{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import Scraper\n",
    "\n",
    "to_scrape = 'https://www.immobiliare.it/'\n",
    "query = 'vendita-case/roma/?criterio=rilevanza&pag={}'\n",
    "\n",
    "data_path = './data'\n",
    "descr_ds_path = data_path + '/description-dataset-raw.tsv'\n",
    "info_ds_path = data_path + '/information-dataset.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Scraper(to_scrape, query_params=query, start_from=1073, sleep=0.15, n_ads=15000, onfile=True, verbose=True).init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to process the description_dataset.tsv\n",
    "# and create a vocabulary\n",
    "import csv\n",
    "from utils import preprocessing_nltk\n",
    "\n",
    "vocabulary = set()\n",
    "doc2voc = {}\n",
    "voc2doc = {}\n",
    "\n",
    "with open(descr_ds_path, 'r') as descr_ds:\n",
    "    reader = csv.reader(descr_ds, delimiter='\\t', quotechar=None)\n",
    "    \n",
    "    for elems in reader:\n",
    "        ad_id = elems[0]\n",
    "        descr = elems[1]\n",
    "        \n",
    "        # process\n",
    "        words = preprocessing_nltk(descr)\n",
    "        vocabulary.update(words)\n",
    "        \n",
    "        # doc2voc\n",
    "        doc2voc[ad_id] = words\n",
    "        \n",
    "        # voc2doc\n",
    "        for w in words:\n",
    "            if not w in voc2doc.keys():\n",
    "                voc2doc[w] = set([ad_id])\n",
    "            else:\n",
    "                voc2doc[w].add(ad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28086\n",
      "['urano', 'numeros', 'modici', 'inverterrilevator', 'tubazioni', 'scritt', 'consultazion', 'intersezioni', 'ringhier', 'chiediamo', 'provvigionezona', 'advisor', 'nn', 'reciproca', 'crocco', 'direttoinfo', 'riorganizzabil', 'dichiarazion', 'appl', 'cucionotto']\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))\n",
    "print(list(vocabulary)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# need to calculate tfidf for each word\n",
    "from utils import tfidf_inverse_index\n",
    "\n",
    "inv_index = tfidf_inverse_index(voc2doc, doc2voc)\n",
    "voc2doc_tfidf = inv_index['voc2doc_tfidf']\n",
    "doc2voc_tfidf = inv_index['doc2voc_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28086\n",
      "28086\n",
      "20013\n",
      "20013\n"
     ]
    }
   ],
   "source": [
    "print(len(voc2doc_tfidf.keys()))\n",
    "print(len(voc2doc.keys()))\n",
    "print(len(doc2voc_tfidf.keys()))\n",
    "print(len(doc2voc.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('67801957', 0.016224790368615277), ('70397474', 0.04020926308743786), ('70238502', 0.028455786184956026), ('65868305', 0.014797008816177133)]\n"
     ]
    }
   ],
   "source": [
    "print(voc2doc_tfidf['albero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = '\\t'\n",
    "NL = '\\n'\n",
    "\n",
    "with open('description-dataset.tsv', 'w') as descr_out:\n",
    "    for doc_id, content in doc2voc.items():\n",
    "        descr_out.write(doc_id + SEP)\n",
    "        \n",
    "        for w in voc2doc.keys():\n",
    "            if w in content:\n",
    "                tfidf = [x[1] for x in voc2doc_tfidf[w] if x[0] == doc_id]\n",
    "                descr_out.write(str(tfidf[0]) + SEP)\n",
    "            else:\n",
    "                descr_out.write(str(0.0) + SEP)\n",
    "        descr_out.write(NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('description-dataset-header.tsv', 'w') as descr_out:\n",
    "    descr_out.write('id' + SEP + SEP.join(voc2doc.keys()) + NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information-dastaset.tsv\n",
    "# clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "info_data = []\n",
    "\n",
    "with open(info_ds_path, 'r') as info_ds:\n",
    "    reader = csv.reader(info_ds, delimiter='\\t', quotechar=None)\n",
    "    \n",
    "    for elems in reader:\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
