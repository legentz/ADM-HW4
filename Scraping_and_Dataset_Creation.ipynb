{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import Scraper\n",
    "\n",
    "to_scrape = 'https://www.immobiliare.it/'\n",
    "query = 'vendita-case/roma/?criterio=rilevanza&pag={}'\n",
    "\n",
    "data_path = './data'\n",
    "descr_ds_path = data_path + '/description-dataset-raw.tsv'\n",
    "info_ds_path = data_path + '/information-dataset.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a reusable python module to easily scrape on file as a stream, to avoid losing any changes even in case of Network Exceptions. Thanks to this, we were able to scrape just over 20k items during 2 scraping sessions only.\n",
    "\n",
    "Then, using terminal tools (awk, cut, wc, cat, ...) we processed and merged our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Scraper(to_scrape, query_params=query, start_from=1, sleep=0.15, n_ads=20000, onfile=True, verbose=True).init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first matrix will have this format: <img src=\"https://latex.codecogs.com/gif.latex?$m_{ij}&space;=&space;value$\" title=\"$m_{ij} = value$\" /> where <img src=\"https://latex.codecogs.com/gif.latex?$i&space;\\in&space;\\{announcement_1,&space;...,&space;announcement_n\\}$\" title=\"$i \\in \\{announcement_1, ..., announcement_n\\}$\" /> and <img src=\"https://latex.codecogs.com/gif.latex?$j&space;\\in&space;\\{price,&space;locali,&space;superficie,&space;bagni,&space;piano&space;\\}$\" title=\"$j \\in \\{price, locali, superficie, bagni, piano \\}$\" />. *n* is the number of the announcements. It's possible that not all the announcements will have all the fields mentioned above, if it's the case don't take it into account. \n",
    "\n",
    "The second matrix will have this format: <img src=\"https://latex.codecogs.com/gif.latex?$m_{ij}&space;=&space;tfIdf_{ij}$\" title=\"$m_{ij} = tfIdf_{ij}$\" /> where <img src=\"https://latex.codecogs.com/gif.latex?$i&space;\\in&space;\\{announcement_1,&space;...,&space;announcement_n\\}$\" title=\"$i \\in \\{announcement_1, ..., announcement_n\\}$\" /> and <img src=\"https://latex.codecogs.com/gif.latex?$j&space;\\in&space;\\{word_1,&space;...,word_m\\}$\" title=\"$j \\in \\{word_1, ...,word_m\\}$\" />. *n* is the number of the announcements and *m* is the cardinality of the vocabulary.\n",
    "\n",
    "Processing the most complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to process the description_dataset.tsv\n",
    "# and create a vocabulary\n",
    "import csv\n",
    "from utils import preprocessing_nltk\n",
    "\n",
    "vocabulary = set()\n",
    "doc2voc = {}\n",
    "voc2doc = {}\n",
    "\n",
    "with open(descr_ds_path, 'r') as descr_ds:\n",
    "    reader = csv.reader(descr_ds, delimiter='\\t', quotechar=None)\n",
    "    \n",
    "    for elems in reader:\n",
    "        ad_id = elems[0]\n",
    "        descr = elems[1]\n",
    "        \n",
    "        # process\n",
    "        words = preprocessing_nltk(descr)\n",
    "        vocabulary.update(words)\n",
    "        \n",
    "        #Â doc2voc\n",
    "        doc2voc[ad_id] = words\n",
    "        \n",
    "        # voc2doc\n",
    "        for w in words:\n",
    "            if not w in voc2doc.keys():\n",
    "                voc2doc[w] = set([ad_id])\n",
    "            else:\n",
    "                voc2doc[w].add(ad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28086\n",
      "['urano', 'numeros', 'modici', 'inverterrilevator', 'tubazioni', 'scritt', 'consultazion', 'intersezioni', 'ringhier', 'chiediamo', 'provvigionezona', 'advisor', 'nn', 'reciproca', 'crocco', 'direttoinfo', 'riorganizzabil', 'dichiarazion', 'appl', 'cucionotto']\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))\n",
    "print(list(vocabulary)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# need to calculate tfidf for each word\n",
    "from utils import tfidf_inverse_index\n",
    "\n",
    "inv_index = tfidf_inverse_index(voc2doc, doc2voc)\n",
    "voc2doc_tfidf = inv_index['voc2doc_tfidf']\n",
    "doc2voc_tfidf = inv_index['doc2voc_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28086\n",
      "28086\n",
      "20013\n",
      "20013\n"
     ]
    }
   ],
   "source": [
    "# check vectors length\n",
    "print(len(voc2doc_tfidf.keys()))\n",
    "print(len(voc2doc.keys()))\n",
    "print(len(doc2voc_tfidf.keys()))\n",
    "print(len(doc2voc.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('67801957', 0.016224790368615277), ('70397474', 0.04020926308743786), ('70238502', 0.028455786184956026), ('65868305', 0.014797008816177133)]\n"
     ]
    }
   ],
   "source": [
    "print(voc2doc_tfidf['albero'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping the processed dataset into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = '\\t'\n",
    "NL = '\\n'\n",
    "\n",
    "with open('description-dataset.tsv', 'w') as descr_out:\n",
    "    for doc_id, content in doc2voc.items():\n",
    "        descr_out.write(doc_id + SEP)\n",
    "        \n",
    "        for w in voc2doc.keys():\n",
    "            if w in content:\n",
    "                tfidf = [x[1] for x in voc2doc_tfidf[w] if x[0] == doc_id]\n",
    "                descr_out.write(str(tfidf[0]) + SEP)\n",
    "            else:\n",
    "                descr_out.write(str(0.0) + SEP)\n",
    "        descr_out.write(NL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
